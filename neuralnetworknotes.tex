\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Neural Network Notes}
\author{Eric Wang}
\date{Start: January 1, 2023}

\begin{document}
\maketitle
\newpage
\section{Andrej Kaparthy: Intro to Neural 
Networks and Backpropagation, Micrograd 
Library}

\begin{itemize}
    \item backpropagation is an algorithm that
allows you to efficiently evaluate the gradient
of a loss function based on the weights of
a neural network
    \item this way, you can tune the weights
to improve the neural network accordingly
    \item going backwards from an ending node,
you can take derivatives with respect to child
node weights, which are necessary information
    \item backpropogation is not dependent on
neural networks, but neural networks utilise
backpropogation
    \item $h = 0.001$, a = 2.0, b = -3.0, 
c = 10.0 and take $d1 = a*b + c$, then take
a = 2.0 + h and find $d2 = *b + c$ which 
then we can take $/{d2-d1}{h}$ to get the
slope with respect to a
\end{itemize}



\end{document}